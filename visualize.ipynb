{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api(timeout=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = api.run(\"pjajal/inference-diffusion-noise-optim/fv5iu18q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(\"eval_results\", \"fu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_history = run.scan_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_log(log):\n",
    "    out = {}\n",
    "    for k, v in log.items():\n",
    "        if isinstance(v, dict):\n",
    "            for k2, v2 in v.items():\n",
    "                out[f\"{k}.{k2}\"] = v2\n",
    "        elif isinstance(v, list):\n",
    "            if len(v) == 1:\n",
    "                out[k] = v[0]\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = []\n",
    "for log in scan_history:\n",
    "    logs.append(flatten_log(log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_prompts = {i: prompt for i, prompt in enumerate(all_data['prompt'].unique())}\n",
    "reverse_unq_prompts = {v: k for k, v in unq_prompts.items()}\n",
    "with open(\"./eval_results/fv5iu18q/prompts.json\", \"w\") as f:\n",
    "    json.dump(unq_prompts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each prompt, extract the row corresponding to the 0-th step and the step with the maximum pop_best_eval\n",
    "best_worst = []\n",
    "for prompt, df_group in all_data.groupby(\"prompt\"):\n",
    "    step_0_row = df_group[df_group[\"step\"] == 0]\n",
    "    max_pop_best_eval_row = df_group.loc[df_group[\"pop_best_eval\"].idxmax()]\n",
    "    idx_prompt = reverse_unq_prompts[prompt]\n",
    "    \n",
    "    save_loc = f\"eval_results/fv5iu18q/{idx_prompt}/\"\n",
    "    os.makedirs(save_loc, exist_ok=True)\n",
    "    for i, row in df_group.iterrows():\n",
    "        step = row['step']\n",
    "        img_save_loc = os.path.join(save_loc, f\"{step}.png\")\n",
    "        if os.path.exists(img_save_loc):\n",
    "            continue\n",
    "        try:\n",
    "            status = run.file(row['best_img.path']).download()\n",
    "            os.renames(status.name, os.path.join(save_loc, f\"{step}.png\"))\n",
    "        except:\n",
    "            print(f\"Failed to download {row['best_img.path']}\")\n",
    "\n",
    "    baseline_save_loc = os.path.join(save_loc, \"baseline.png\")\n",
    "    max_save_loc = os.path.join(save_loc, \"max.png\")\n",
    "    \n",
    "    if not os.path.exists(baseline_save_loc):\n",
    "        baseline_status = run.file(step_0_row['best_img.path'].item()).download()\n",
    "        os.renames(baseline_status.name, os.path.join(save_loc, \"baseline.png\"))\n",
    "\n",
    "    if not os.path.exists(max_save_loc):\n",
    "        max_status = run.file(max_pop_best_eval_row['best_img.path']).download()\n",
    "        os.renames(max_status.name, os.path.join(save_loc, \"max.png\"))\n",
    "    best_worst.append({\"prompt\": prompt, \"baseline\": step_0_row['pop_best_eval'].item(), \"best\": max_pop_best_eval_row['pop_best_eval'].item()})\n",
    "    print(\n",
    "        f\"Prompt: {prompt}, Step 0: {step_0_row['pop_best_eval'].item()}, Max Pop Best Eval: {max_pop_best_eval_row['pop_best_eval'].item()}\"\n",
    "    )\n",
    "\n",
    "best_worst_results = pd.DataFrame(best_worst)\n",
    "best_worst_results.to_csv(\"eval_results/fv5iu18q/best_worst_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import hpsv2\n",
    "import torch\n",
    "import ImageReward\n",
    "import os\n",
    "import json\n",
    "from diffusers.utils import pt_to_pil, numpy_to_pil\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from transformers import (\n",
    "    CLIPProcessor,\n",
    "    CLIPImageProcessor,\n",
    "    CLIPModel,\n",
    "    AutoModel,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_reward = ImageReward.load(\"ImageReward-v1.0\")\n",
    "img_reward = img_reward.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_loc = \"eval_results/fv5iu18q/\"\n",
    "prompts_file = os.path.join(eval_results_loc, \"prompts.json\")\n",
    "best_worst_file = os.path.join(eval_results_loc, \"best_worst_results.csv\")\n",
    "\n",
    "with open(prompts_file, \"r\") as f:\n",
    "    prompt_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = []\n",
    "for idx, prompt in prompt_dict.items():\n",
    "    img_save_loc = os.path.join(eval_results_loc, idx)\n",
    "\n",
    "    path_score_list = []\n",
    "    for img_path in glob(os.path.join(img_save_loc, \"[0-9]*.png\")):\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        img_reward_score = img_reward.score(prompt, img)\n",
    "        path_score_list.append((img_path, img_reward_score))\n",
    "\n",
    "    highest_score = max(path_score_list, key=lambda x: x[1])\n",
    "    lowest_score = min(path_score_list, key=lambda x: x[1])\n",
    "    \n",
    "    \n",
    "    baseline_score_path = os.path.join(img_save_loc, \"baseline.png\")\n",
    "    baseline_img = Image.open(baseline_score_path)\n",
    "    baseline_img_score = img_reward.score(prompt, baseline_img)\n",
    "\n",
    "    measurements.append(\n",
    "        {\n",
    "            \"prompt\": prompt,\n",
    "            \"highest_score\": highest_score[1],\n",
    "            \"highest_score_path\": highest_score[0],\n",
    "            \"lowest_score\": lowest_score[1],\n",
    "            \"lowest_score_path\": lowest_score[0],\n",
    "            \"baseline_score\": baseline_img_score,\n",
    "            \"baseline_score_path\": baseline_score_path,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(measurements).to_csv(\"eval_results/fv5iu18q/measurements.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = pd.read_csv(\"eval_results/fv5iu18q/measurements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "clip_model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-large-patch14\"\n",
    ").eval().to(device=\"mps\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def clip_score(processor, clip_model, prompt, img):\n",
    "    inputs = processor(text=[prompt], images=[img], return_tensors=\"pt\", padding=True).to(device=\"mps\")\n",
    "    outputs = clip_model(**inputs)\n",
    "    score = outputs[0][0]\n",
    "    return score.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AestheticMLP(nn.Module):\n",
    "    def __init__(self, input_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ### Normalize CLIP embedding\n",
    "        l2 = torch.linalg.vector_norm(x, dim=-1, keepdim=True)\n",
    "        ### NEXT LINE MESSES WITH DNO GRADIENT! There are conflicting implementations (some have this, some dont)\n",
    "        ### Opting to leave it out\n",
    "        # l2[l2 == 0] = 1\n",
    "        x = x / l2\n",
    "        ### Apply MLP, return score\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "### Load CLIP model and processor\n",
    "aesthetic_processor = CLIPProcessor(\n",
    "    CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\"),\n",
    "    AutoTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\"),\n",
    ")\n",
    "\n",
    "aesthetic_clip_model = (\n",
    "    CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").eval().to(device=\"mps\")\n",
    ")\n",
    "\n",
    "### Load linear classifier\n",
    "aesthetic_mlp = AestheticMLP(768)\n",
    "aesthetic_mlp.load_state_dict(\n",
    "    torch.load(os.path.join(\"./\", \"aesthetic_mlp.pth\"), map_location=\"cpu\"),\n",
    "    strict=False,\n",
    "    assign=True,\n",
    ")\n",
    "aesthetic_mlp.eval().to(device=\"mps\")\n",
    "\n",
    "def aesthetic_score(img):\n",
    "    inputs = aesthetic_processor(\n",
    "        images=img,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        do_rescale=False,\n",
    "    ).to(device=\"mps\")\n",
    "\n",
    "    clip_image_embeddings = aesthetic_clip_model.get_image_features(**inputs)\n",
    "    score = aesthetic_mlp(clip_image_embeddings)\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_measurements = []\n",
    "for i, row in tqdm(measurements.iterrows()):\n",
    "    prompt = row[\"prompt\"]\n",
    "    highest_score_img_reward = row[\"highest_score\"]\n",
    "    lowest_score_img_reward = row[\"lowest_score\"]\n",
    "    baseline_score_img_reward = row[\"baseline_score\"]\n",
    "    highest_score_path = row[\"highest_score_path\"]\n",
    "    lowest_score_path = row[\"lowest_score_path\"]\n",
    "    baseline_score_path = row[\"baseline_score_path\"]\n",
    "\n",
    "    highest_score_img = Image.open(highest_score_path)\n",
    "    lowest_score_img = Image.open(lowest_score_path)\n",
    "    baseline_score_img = Image.open(baseline_score_path)\n",
    "\n",
    "    # HPSV2\n",
    "    with torch.no_grad():\n",
    "        highest_score_hpsv = hpsv2.score(highest_score_img, prompt)\n",
    "        lowest_score_hpsv = hpsv2.score(lowest_score_img, prompt)\n",
    "        baseline_score_hpsv = hpsv2.score(baseline_score_img, prompt)\n",
    "\n",
    "    # CLIP\n",
    "    highest_score_clip = None\n",
    "    lowest_score_clip = None\n",
    "    baseline_score_clip = None\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            highest_score_clip = clip_score(\n",
    "                processor, clip_model, prompt, highest_score_img\n",
    "            )\n",
    "            lowest_score_clip = clip_score(\n",
    "                processor, clip_model, prompt, lowest_score_img\n",
    "            )\n",
    "            baseline_score_clip = clip_score(\n",
    "                processor, clip_model, prompt, baseline_score_img\n",
    "            )\n",
    "    except:\n",
    "        print(f\"Failed to score {prompt} with CLIP\")\n",
    "\n",
    "    # Aesthetic\n",
    "    with torch.no_grad():\n",
    "        highest_score_aesthetic = aesthetic_score(highest_score_img)\n",
    "        lowest_score_aesthetic = aesthetic_score(lowest_score_img)\n",
    "        baseline_score_aesthetic = aesthetic_score(baseline_score_img)\n",
    "\n",
    "    all_measurements.append(\n",
    "        {\n",
    "            \"prompt\": prompt,\n",
    "            \"highest_score_img_reward\": highest_score_img_reward,\n",
    "            \"lowest_score_img_reward\": lowest_score_img_reward,\n",
    "            \"baseline_score_img_reward\": baseline_score_img_reward,\n",
    "            \"highest_score_hpsv\": highest_score_hpsv,\n",
    "            \"lowest_score_hpsv\": lowest_score_hpsv,\n",
    "            \"baseline_score_hpsv\": baseline_score_hpsv,\n",
    "            \"highest_score_clip\": highest_score_clip,\n",
    "            \"lowest_score_clip\": lowest_score_clip,\n",
    "            \"baseline_score_clip\": baseline_score_clip,\n",
    "            \"highest_score_aesthetic\": highest_score_aesthetic,\n",
    "            \"lowest_score_aesthetic\": lowest_score_aesthetic,\n",
    "            \"baseline_score_aesthetic\": baseline_score_aesthetic,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_v(v):\n",
    "    if isinstance(v, list):\n",
    "        return handle_v(v[0])\n",
    "    if isinstance(v, np.float32):\n",
    "        return v.item()\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        # Handle torch tensors\n",
    "        if v.numel() == 1:\n",
    "            return v.item()\n",
    "        else:\n",
    "            # Return the numpy array if it has more than one element\n",
    "            return v.cpu().numpy()\n",
    "    return v\n",
    "\n",
    "\n",
    "pd.DataFrame(\n",
    "    map(lambda x: {k: handle_v(v) for k, v in x.items()}, all_measurements)\n",
    ").to_csv(\"eval_results/fv5iu18q/all_measurements.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meas = pd.read_csv(\"eval_results/fv5iu18q/all_measurements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meas['high_vs_baseline_img_reward'] = all_meas['highest_score_img_reward'] - all_meas['baseline_score_img_reward']\n",
    "all_meas['high_vs_baseline_hpsv'] = all_meas['highest_score_hpsv'] - all_meas['baseline_score_hpsv']\n",
    "all_meas['high_vs_baseline_clip'] = all_meas['highest_score_clip'] - all_meas['baseline_score_clip']\n",
    "all_meas['high_vs_baseline_aesthetic'] = all_meas['highest_score_aesthetic'] - all_meas['baseline_score_aesthetic']\n",
    "\n",
    "all_meas['low_vs_baseline_img_reward'] = all_meas['lowest_score_img_reward'] - all_meas['baseline_score_img_reward']\n",
    "all_meas['low_vs_baseline_hpsv'] = all_meas['lowest_score_hpsv'] - all_meas['baseline_score_hpsv']\n",
    "all_meas['low_vs_baseline_clip'] = all_meas['lowest_score_clip'] - all_meas['baseline_score_clip']\n",
    "all_meas['low_vs_baseline_aesthetic'] = all_meas['lowest_score_aesthetic'] - all_meas['baseline_score_aesthetic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meas[['high_vs_baseline_img_reward', 'high_vs_baseline_hpsv', 'high_vs_baseline_clip', \"high_vs_baseline_aesthetic\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meas[['low_vs_baseline_img_reward', 'low_vs_baseline_hpsv', 'low_vs_baseline_clip', \"low_vs_baseline_aesthetic\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from fitness.verifiers.gemini_verifier import GeminiVerifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GEMINI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifier = GeminiVerifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"eval_results/fv5iu18q/measurements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_measurements = []\n",
    "for i, row in df.iterrows():\n",
    "    if i <= 137:\n",
    "        continue\n",
    "    time.sleep(8)\n",
    "    inputs = verifier.prepare_inputs(\n",
    "        images=[\n",
    "            row[\"lowest_score_path\"],\n",
    "            row[\"baseline_score_path\"],\n",
    "            row[\"highest_score_path\"],\n",
    "        ],\n",
    "        prompts=[row[\"prompt\"]] * 3,\n",
    "    )\n",
    "    outputs = verifier.score(inputs)\n",
    "    lowest, baseline, highest = outputs\n",
    "\n",
    "    lowest_flattened = {f\"lowest_{k}\": v['score'] for k, v in lowest.items()}\n",
    "    baseline_flattened = {f\"baseline_{k}\": v['score'] for k, v in baseline.items()}\n",
    "    highest_flattened = {f\"highest_{k}\": v['score'] for k, v in highest.items()}\n",
    "    verified_measurements.append(\n",
    "        {\n",
    "            \"prompt\": row[\"prompt\"],\n",
    "            **lowest_flattened,\n",
    "            **baseline_flattened,\n",
    "            **highest_flattened,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.read_csv(\"results/fv5iu18q.csv\")\n",
    "gemini_evals = pd.read_csv(\"eval_results/fv5iu18q/verified_measurements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.join(gemini_evals.set_index(\"prompt\"), on=\"prompt\").to_csv(\"results/fv5iu18q+llm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_evals['highest_overall_score'].mean() - gemini_evals['lowest_overall_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-traj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
