{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api(timeout=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = api.run(\"pjajal/inference-diffusion-noise-optim/fv5iu18q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_history = run.scan_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_log(log):\n",
    "    out = {}\n",
    "    for k, v in log.items():\n",
    "        if isinstance(v, dict):\n",
    "            for k2, v2 in v.items():\n",
    "                out[f\"{k}.{k2}\"] = v2\n",
    "        elif isinstance(v, list):\n",
    "            if len(v) == 1:\n",
    "                out[k] = v[0]\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = []\n",
    "for log in scan_history:\n",
    "    logs.append(flatten_log(log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_prompts = {i: prompt for i, prompt in enumerate(all_data['prompt'].unique())}\n",
    "reverse_unq_prompts = {v: k for k, v in unq_prompts.items()}\n",
    "with open(\"./eval_results/fv5iu18q/prompts.json\", \"w\") as f:\n",
    "    json.dump(unq_prompts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each prompt, extract the row corresponding to the 0-th step and the step with the maximum pop_best_eval\n",
    "best_worst = []\n",
    "for prompt, df_group in all_data.groupby(\"prompt\"):\n",
    "    step_0_row = df_group[df_group[\"step\"] == 0]\n",
    "    max_pop_best_eval_row = df_group.loc[df_group[\"pop_best_eval\"].idxmax()]\n",
    "    idx_prompt = reverse_unq_prompts[prompt]\n",
    "    \n",
    "    save_loc = f\"eval_results/fv5iu18q/{idx_prompt}/\"\n",
    "    os.makedirs(save_loc, exist_ok=True)\n",
    "    for i, row in df_group.iterrows():\n",
    "        step = row['step']\n",
    "        img_save_loc = os.path.join(save_loc, f\"{step}.png\")\n",
    "        if os.path.exists(img_save_loc):\n",
    "            continue\n",
    "        try:\n",
    "            status = run.file(row['best_img.path']).download()\n",
    "            os.renames(status.name, os.path.join(save_loc, f\"{step}.png\"))\n",
    "        except:\n",
    "            print(f\"Failed to download {row['best_img.path']}\")\n",
    "\n",
    "    baseline_save_loc = os.path.join(save_loc, \"baseline.png\")\n",
    "    max_save_loc = os.path.join(save_loc, \"max.png\")\n",
    "    \n",
    "    if not os.path.exists(baseline_save_loc):\n",
    "        baseline_status = run.file(step_0_row['best_img.path'].item()).download()\n",
    "        os.renames(baseline_status.name, os.path.join(save_loc, \"baseline.png\"))\n",
    "\n",
    "    if not os.path.exists(max_save_loc):\n",
    "        max_status = run.file(max_pop_best_eval_row['best_img.path']).download()\n",
    "        os.renames(max_status.name, os.path.join(save_loc, \"max.png\"))\n",
    "    best_worst.append({\"prompt\": prompt, \"baseline\": step_0_row['pop_best_eval'].item(), \"best\": max_pop_best_eval_row['pop_best_eval'].item()})\n",
    "    print(\n",
    "        f\"Prompt: {prompt}, Step 0: {step_0_row['pop_best_eval'].item()}, Max Pop Best Eval: {max_pop_best_eval_row['pop_best_eval'].item()}\"\n",
    "    )\n",
    "\n",
    "best_worst_results = pd.DataFrame(best_worst)\n",
    "best_worst_results.to_csv(\"eval_results/fv5iu18q/best_worst_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import hpsv2\n",
    "import torch\n",
    "import ImageReward\n",
    "import os\n",
    "import json\n",
    "from diffusers.utils import pt_to_pil, numpy_to_pil\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from transformers import (\n",
    "    CLIPProcessor,\n",
    "    CLIPImageProcessor,\n",
    "    CLIPModel,\n",
    "    AutoModel,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import pandas as pd\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_reward = ImageReward.load(\"ImageReward-v1.0\")\n",
    "img_reward = img_reward.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_loc = \"eval_results/fv5iu18q/\"\n",
    "prompts_file = os.path.join(eval_results_loc, \"prompts.json\")\n",
    "best_worst_file = os.path.join(eval_results_loc, \"best_worst_results.csv\")\n",
    "\n",
    "with open(prompts_file, \"r\") as f:\n",
    "    prompt_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = []\n",
    "for idx, prompt in prompt_dict.items():\n",
    "    img_save_loc = os.path.join(eval_results_loc, idx)\n",
    "\n",
    "    path_score_list = []\n",
    "    for img_path in glob(os.path.join(img_save_loc, \"[0-9]*.png\")):\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        img_reward_score = img_reward.score(prompt, img)\n",
    "        path_score_list.append((img_path, img_reward_score))\n",
    "\n",
    "    highest_score = max(path_score_list, key=lambda x: x[1])\n",
    "    lowest_score = min(path_score_list, key=lambda x: x[1])\n",
    "    \n",
    "    \n",
    "    baseline_score_path = os.path.join(img_save_loc, \"baseline.png\")\n",
    "    baseline_img = Image.open(baseline_score_path)\n",
    "    baseline_img_score = img_reward.score(prompt, baseline_img)\n",
    "\n",
    "    measurements.append(\n",
    "        {\n",
    "            \"prompt\": prompt,\n",
    "            \"highest_score\": highest_score[1],\n",
    "            \"highest_score_path\": highest_score[0],\n",
    "            \"lowest_score\": lowest_score[1],\n",
    "            \"lowest_score_path\": lowest_score[0],\n",
    "            \"baseline_score\": baseline_img_score,\n",
    "            \"baseline_score_path\": baseline_score_path,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(measurements).to_csv(\"eval_results/fv5iu18q/measurements.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = pd.read_csv(\"eval_results/fv5iu18q/measurements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "clip_model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-large-patch14\"\n",
    ").eval().to(device=\"mps\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def clip_score(processor, clip_model, prompt, img):\n",
    "    inputs = processor(text=[prompt], images=[img], return_tensors=\"pt\", padding=True).to(device=\"mps\")\n",
    "    outputs = clip_model(**inputs)\n",
    "    score = outputs[0][0]\n",
    "    return score.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AestheticMLP(nn.Module):\n",
    "    def __init__(self, input_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ### Normalize CLIP embedding\n",
    "        l2 = torch.linalg.vector_norm(x, dim=-1, keepdim=True)\n",
    "        ### NEXT LINE MESSES WITH DNO GRADIENT! There are conflicting implementations (some have this, some dont)\n",
    "        ### Opting to leave it out\n",
    "        # l2[l2 == 0] = 1\n",
    "        x = x / l2\n",
    "        ### Apply MLP, return score\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "### Load CLIP model and processor\n",
    "aesthetic_processor = CLIPProcessor(\n",
    "    CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\"),\n",
    "    AutoTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\"),\n",
    ")\n",
    "\n",
    "aesthetic_clip_model = (\n",
    "    CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").eval().to(device=\"mps\")\n",
    ")\n",
    "\n",
    "### Load linear classifier\n",
    "aesthetic_mlp = AestheticMLP(768)\n",
    "aesthetic_mlp.load_state_dict(\n",
    "    torch.load(os.path.join(\"./\", \"aesthetic_mlp.pth\"), map_location=\"cpu\"),\n",
    "    strict=False,\n",
    "    assign=True,\n",
    ")\n",
    "aesthetic_mlp.eval().to(device=\"mps\")\n",
    "\n",
    "def aesthetic_score(img):\n",
    "    inputs = aesthetic_processor(\n",
    "        images=img,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        do_rescale=False,\n",
    "    ).to(device=\"mps\")\n",
    "\n",
    "    clip_image_embeddings = aesthetic_clip_model.get_image_features(**inputs)\n",
    "    score = aesthetic_mlp(clip_image_embeddings)\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to score An instqrumemnt used for cutting cloth, paper, axdz othr thdin mteroial, consamistng of two blades lad one on tvopb of the other and fhastned in tle mixdqdjle so as to bllow them txo be pened and closed by thumb and fitngesr inserted tgrough rings on kthe end oc thei vatndlzes. with CLIP\n",
      "Failed to score A ldarge keybord msical instroument lwith a woden case enmclosig a qsouvnkboajrd and mfgtal strivgf, which are strucrk b hammrs when the nels are depresdsmed.f lhe strsingsj' vibration ie stopped by damperds when the keys re released and can bce regulavewdd for lengh and vnolume y two or three pedalvs. with CLIP\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# HPSV2\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 17\u001b[0m     highest_score_hpsv \u001b[38;5;241m=\u001b[39m \u001b[43mhpsv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhighest_score_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     lowest_score_hpsv \u001b[38;5;241m=\u001b[39m hpsv2\u001b[38;5;241m.\u001b[39mscore(lowest_score_img, prompt)\n\u001b[1;32m     19\u001b[0m     baseline_score_hpsv \u001b[38;5;241m=\u001b[39m hpsv2\u001b[38;5;241m.\u001b[39mscore(baseline_score_img, prompt)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/diffusion-traj/lib/python3.12/site-packages/hpsv2/__init__.py:112\u001b[0m, in \u001b[0;36mscore\u001b[0;34m(imgs_path, prompt, hps_version)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Score the image and prompt\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    list: matching scores for images and prompt\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m img_score \u001b[38;5;28;01mas\u001b[39;00m scr\n\u001b[0;32m--> 112\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mscr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhps_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhps_version\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/diffusion-traj/lib/python3.12/site-packages/hpsv2/img_score.py:55\u001b[0m, in \u001b[0;36mscore\u001b[0;34m(img_path, prompt, cp, hps_version)\u001b[0m\n\u001b[1;32m     52\u001b[0m     cp \u001b[38;5;241m=\u001b[39m huggingface_hub\u001b[38;5;241m.\u001b[39mhf_hub_download(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxswu/HPSv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, hps_version_map[hps_version])\n\u001b[1;32m     54\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(cp, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViT-H-14\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     57\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/diffusion-traj/lib/python3.12/site-packages/torch/nn/modules/module.py:2564\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         out \u001b[38;5;241m=\u001b[39m hook(module, incompatible_keys)\n\u001b[1;32m   2558\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m   2559\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2560\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2561\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit should be done inplace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2562\u001b[0m         )\n\u001b[0;32m-> 2564\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/diffusion-traj/lib/python3.12/site-packages/torch/nn/modules/module.py:2552\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2546\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2547\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2548\u001b[0m             k: v\n\u001b[1;32m   2549\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2550\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)\n\u001b[1;32m   2551\u001b[0m         }\n\u001b[0;32m-> 2552\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2555\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/diffusion-traj/lib/python3.12/site-packages/torch/nn/modules/module.py:2552\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2546\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2547\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2548\u001b[0m             k: v\n\u001b[1;32m   2549\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2550\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)\n\u001b[1;32m   2551\u001b[0m         }\n\u001b[0;32m-> 2552\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2555\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "    \u001b[0;31m[... skipping similar frames: Module.load_state_dict.<locals>.load at line 2552 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/diffusion-traj/lib/python3.12/site-packages/torch/nn/modules/module.py:2552\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2546\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2547\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2548\u001b[0m             k: v\n\u001b[1;32m   2549\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2550\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)\n\u001b[1;32m   2551\u001b[0m         }\n\u001b[0;32m-> 2552\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2555\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/diffusion-traj/lib/python3.12/site-packages/torch/nn/modules/module.py:2535\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[1;32m   2534\u001b[0m     local_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massign_to_params_buffers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m assign\n\u001b[0;32m-> 2535\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[43m    \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/diffusion-traj/lib/python3.12/site-packages/torch/nn/modules/module.py:2441\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2439\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2440\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2441\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2443\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswapping\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopying\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_measurements = []\n",
    "for i, row in measurements.iterrows():\n",
    "    prompt = row[\"prompt\"]\n",
    "    highest_score_img_reward = row[\"highest_score\"]\n",
    "    lowest_score_img_reward = row[\"lowest_score\"]\n",
    "    baseline_score_img_reward = row[\"baseline_score\"]\n",
    "    highest_score_path = row[\"highest_score_path\"]\n",
    "    lowest_score_path = row[\"lowest_score_path\"]\n",
    "    baseline_score_path = row[\"baseline_score_path\"]\n",
    "\n",
    "    highest_score_img = Image.open(highest_score_path)\n",
    "    lowest_score_img = Image.open(lowest_score_path)\n",
    "    baseline_score_img = Image.open(baseline_score_path)\n",
    "\n",
    "    # HPSV2\n",
    "    with torch.no_grad():\n",
    "        highest_score_hpsv = hpsv2.score(highest_score_img, prompt)\n",
    "        lowest_score_hpsv = hpsv2.score(lowest_score_img, prompt)\n",
    "        baseline_score_hpsv = hpsv2.score(baseline_score_img, prompt)\n",
    "\n",
    "    # CLIP\n",
    "    highest_score_clip = None\n",
    "    lowest_score_clip = None\n",
    "    baseline_score_clip = None\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            highest_score_clip = clip_score(\n",
    "                processor, clip_model, prompt, highest_score_img\n",
    "            )\n",
    "            lowest_score_clip = clip_score(\n",
    "                processor, clip_model, prompt, lowest_score_img\n",
    "            )\n",
    "            baseline_score_clip = clip_score(\n",
    "                processor, clip_model, prompt, baseline_score_img\n",
    "            )\n",
    "    except:\n",
    "        print(f\"Failed to score {prompt} with CLIP\")\n",
    "\n",
    "    # Aesthetic\n",
    "    with torch.no_grad():\n",
    "        highest_score_aesthetic = aesthetic_score(highest_score_img)\n",
    "        lowest_score_aesthetic = aesthetic_score(lowest_score_img)\n",
    "        baseline_score_aesthetic = aesthetic_score(baseline_score_img)\n",
    "\n",
    "    all_measurements.append(\n",
    "        {\n",
    "            \"prompt\": prompt,\n",
    "            \"highest_score_img_reward\": highest_score_img_reward,\n",
    "            \"lowest_score_img_reward\": lowest_score_img_reward,\n",
    "            \"baseline_score_img_reward\": baseline_score_img_reward,\n",
    "            \"highest_score_hpsv\": highest_score_hpsv,\n",
    "            \"lowest_score_hpsv\": lowest_score_hpsv,\n",
    "            \"baseline_score_hpsv\": baseline_score_hpsv,\n",
    "            \"highest_score_clip\": highest_score_clip,\n",
    "            \"lowest_score_clip\": lowest_score_clip,\n",
    "            \"baseline_score_clip\": baseline_score_clip,\n",
    "            \"highest_score_aesthetic\": highest_score_aesthetic,\n",
    "            \"lowest_score_aesthetic\": lowest_score_aesthetic,\n",
    "            \"baseline_score_aesthetic\": baseline_score_aesthetic,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_v(v):\n",
    "    if isinstance(v, list):\n",
    "        return handle_v(v[0])\n",
    "    if isinstance(v, np.float32):\n",
    "        return v.item()\n",
    "    return v\n",
    "    \n",
    "\n",
    "pd.DataFrame(map(lambda x: {k: handle_v(v) for k, v in x.items()}, all_measurements)).to_csv(\"eval_results/fv5iu18q/all_measurements.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meas = pd.read_csv(\"eval_results/fv5iu18q/all_measurements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meas['high_vs_baseline_img_reward'] = all_meas['highest_score_img_reward'] - all_meas['baseline_score_img_reward']\n",
    "all_meas['high_vs_baseline_hpsv'] = all_meas['highest_score_hpsv'] - all_meas['baseline_score_hpsv']\n",
    "all_meas['high_vs_baseline_clip'] = all_meas['highest_score_clip'] - all_meas['baseline_score_clip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meas[['high_vs_baseline_img_reward', 'high_vs_baseline_hpsv', 'high_vs_baseline_clip']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff-traf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
